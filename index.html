<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Blog post about testing Real-Time Reinforcement Learning in Duckiematrix - A Duckietown Project">
    <title>Real-Time RL in Duckiematrix | Duckietown Project</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <header class="header">
        <nav class="nav">
            <div class="nav-container">
                <a href="#" class="logo">Duckietown</a>
                <ul class="nav-links">
                    <li><a href="#abstract">Abstract</a></li>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#theory">Theory</a></li>
                    <li><a href="#methodology">Methodology</a></li>
                    <li><a href="#experiments">Experiments</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#conclusion">Conclusion</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <main class="main-content">
        <article class="blog-post">
            <!-- Hero Section -->
            <section class="hero">
                <div class="hero-content">
                    <span class="category-badge">Research Project</span>
                    <h1 class="hero-title">Testing Real-Time Reinforcement Learning in Duckiematrix</h1>
                    <p class="hero-subtitle">Exploring the challenges and opportunities of real-time RL in simulated autonomous driving environments</p>
                    <div class="hero-meta">
                        <span class="author">Guillaume Gagné-Labelle, Gabriel Sasseville and Nicolas Bosteels</span>
                        <span class="date">2025</span>
                    </div>
                </div>
            </section>

            <!-- Abstract -->
            <section id="abstract" class="content-section">
                <h2>Abstract</h2>
                <p>
                    Reinforcement Learning (RL) algorithms often model agent-environment interaction in an idealized manner: the agent observes the 
                    state of the world, selects an action, the environment reacts instantly, and the agent receives a reward. This abstraction 
                    is powerful, but, in contunuous-time scenarios, neglects a critical assumption: time. In real-world autonomous systems, 
                    particularly in the self-driving vehicle domain, time is never negligible. Sensing, decision-making and actuation all introduce a 
                    delay. When the environment is dynamic, these delays break the fundamental assumptions behind classical RL algorithms and Markov 
                    Decision Process (MDP) modelling. This project explores this gap through a pragmatic approach: How can we model dynamic scenarios? 
                    How does the induced delay affect the algorithm performance? How should we measure this performance difference? 
                    Our study revealed that action conditioning significantly improves agent performance in the presence of computation delays. At a 0.5s delay, Real-Time RL (with action conditioning) achieves an evaluation reward of 27.77 ± 0.039, compared to 19.11 ± 11.097 for classical RL. However, both approaches fail completely at delays of 1.0s, with evaluation rewards near -1.0 and episode lengths around 6-10 steps, indicating that the task becomes intractable at this delay threshold. The results demonstrate that Real-Time RL techniques can partially compensate for computation delays, but performance degradation is inevitable as delays increase beyond critical thresholds.
                </p>
            </section>

            <!-- Introduction -->
            <section id="introduction" class="content-section">
                <h2>Introduction</h2>
                <p>
                    Reinforcement Learning (RL) has shown remarkable success in various domains, from game playing to robotics. 
                    However, deploying RL algorithms in real-time scenarios presents unique challenges, particularly in safety-critical 
                    applications like autonomous driving. Traditional RL algorithms assume that the environment remains static during 
                    action selection, which is often not the case in real-world applications where computation time introduces delays.
                </p>
                <p>
                    This blog post explores our investigation of Real-Time Reinforcement Learning in the Duckietown simulation environment. 
                    We examine how significant delays in action computation—specifically, the time required for a neural network forward 
                    pass to sample an action—impact agent performance. By simulating these delays in Duckietown's gym mode, we can 
                    systematically study the effects of real-time constraints on RL algorithms.
                </p>
            </section>

            <!-- Theory Section -->
            <section id="theory" class="content-section">
                <h2>Theory: Real-Time Reinforcement Learning</h2>
                
                <h3>The Problem with Classical MDPs</h3>
                <p>
                    Markov Decision Processes (MDPs), the mathematical framework underlying most algorithms in Reinforcement Learning, 
                    are often used in a way that wrongfully assumes that the state of an agent's environment does not change during 
                    action selection. As noted by Ramstedt and Pal in their work on Real-Time RL <a href="https://arxiv.org/abs/1911.04448" target="_blank" class="citation">[1]</a>, this assumption breaks down 
                    in real-world scenarios where computation time is non-negligible.
                </p>
                <p>
                    In classical RL, the typical interaction loop assumes:
                </p>
                <ol class="numbered-list">
                    <li>Agent observes state <em>s<sub>t</sub></em></li>
                    <li>Agent computes action <em>a<sub>t</sub></em> (assumed instantaneous)</li>
                    <li>Environment transitions to state <em>s<sub>t+1</sub></em></li>
                </ol>
                <p>
                    However, in real-time scenarios, there is a delay <em>δ</em> between observing the state and executing the action. 
                    During this delay, the environment continues to evolve. For example, when the agent observes state <em>s<sub>t-1</sub></em> 
                    and computes action <em>a<sub>t-1</sub></em>, by the time this action is actually applied, the environment has advanced 
                    to state <em>s<sub>t</sub></em>. This mismatch can lead to suboptimal or even dangerous behavior, especially in 
                    safety-critical applications.
                </p>

                <h3>The Real-Time MDP Formulation</h3>
                <p>
                    Ramstedt and Pal introduced a new framework where states and actions evolve simultaneously. The key insight is to 
                    modify the MDP formulation to account for the temporal delay between observation and action execution.
                </p>
                <p>
                    In our implementation, we address this by conditioning the policy on both the previous state and the previous action 
                    when sampling a new action. This modification allows the policy to learn to predict state evolution intrinsically 
                    within the model, effectively compensating for the advancing of the state during action computation time.
                </p>
                <p>
                    Formally, instead of the classical policy <em>π(a<sub>t</sub> | s<sub>t</sub>)</em>, we condition the policy on the 
                    previous state and previous action to predict the current action:
                </p>
                <div class="math-formula">
                    <p><em>π(a<sub>t</sub> | s<sub>t-1</sub>, a<sub>t-1</sub>)</em></p>
                </div>
                <p>
                    By conditioning on both <em>s<sub>t-1</sub></em> and <em>a<sub>t-1</sub></em>, the policy can learn to anticipate how 
                    the state will evolve during the computation delay, effectively learning an internal model of the environment dynamics. 
                    This enables the agent to select actions that will be appropriate for the advanced state <em>s<sub>t</sub></em> that exists 
                    when the action is actually executed, rather than the state <em>s<sub>t-1</sub></em> that existed when the action 
                    computation began.
                </p>
                <p>
                    This approach is particularly relevant for neural network policies (such as those used in Soft Actor-Critic (SAC)), 
                    where the forward pass through the policy network introduces a measurable delay. The policy network can learn to 
                    incorporate information about the previous action to make better predictions about future states.
                </p>
            </section>

            <!-- Methodology -->
            <section id="methodology" class="content-section">
                <h2>Methodology</h2>
                
                <h3>Research Questions</h3>
                <p>
                    Our experiments address the following key research questions:
                </p>
                <ol class="numbered-list">
                    <li><strong>How should we measure the gap in performance?</strong> We measure performance using evaluation rewards (average reward over evaluation episodes) and episode lengths, comparing both training and evaluation metrics across different delay values.</li>
                    <li><strong>Does maximum performance degrade with delays?</strong> Yes. Our results show that as delays increase, both classical RL and Real-Time RL experience performance degradation, with classical RL failing completely at 0.1s delay.</li>
                    <li><strong>From which delay does the task become impossible?</strong> Our experiments reveal that both approaches completely fail at 1.0s delays, with evaluation rewards near -1.0 and episode lengths around 6-10 steps, indicating that delays beyond approximately 0.5-1.0s make the task intractable.</li>
                    <li><strong>Can action conditioning compensate for computation delays?</strong> Yes. Real-Time RL with action conditioning consistently outperforms classical RL, with improvements ranging from 5.9% at small delays (0.01s) to 45.3% at larger delays (0.5s), and can successfully learn in scenarios where classical RL fails (0.1s delay).</li>
                </ol>

                <h3>Duckietown Environment</h3>
                <p>
                    Duckietown provides a realistic simulation platform for autonomous vehicles, allowing researchers to test and 
                    validate algorithms in a controlled environment. The platform offers two main modes of operation:
                </p>
                <ul class="feature-list">
                    <li><strong>Gym Mode:</strong> A standard OpenAI Gym interface that allows for controlled, step-by-step execution 
                        where we can artificially introduce delays to simulate computation time.</li>
                    <li><strong>Real-Time Mode:</strong> A mode that runs in real-time, more closely mimicking real-world deployment 
                        scenarios where actions must be computed within strict time constraints.</li>
                </ul>
                <p>
                    For our experiments, we primarily use gym mode, which allows us to systematically control and vary the delay 
                    between state observation and action execution. This enables us to study the impact of different delay magnitudes 
                    on agent performance.
                </p>

                <h3>Simulating Action Computation Delay</h3>
                <p>
                    In our experiments, we simulate the delay introduced by neural network forward passes (such as those in SAC's policy 
                    network) by fixing specific time delays in Duckietown's gym mode. When an agent requests an action, we introduce 
                    a controlled delay before the action is applied to the environment. During this delay, the environment continues 
                    to evolve, simulating the real-world scenario where computation time allows the state to advance.
                </p>
                <p>
                    This approach allows us to:
                </p>
                <ul class="feature-list">
                    <li>Systematically study the impact of different delay magnitudes</li>
                    <li>Compare classical RL (without action conditioning) against Real-Time RL (with action conditioning)</li>
                    <li>Test both fixed and variable delay scenarios</li>
                </ul>

                <h3>Hyperparameter Tuning</h3>
                <p>
                    Before conducting our main experiments, we perform comprehensive hyperparameter tuning to ensure fair comparisons 
                    between different approaches. The following table outlines the key hyperparameters we tune:
                </p>
                
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Hyperparameter</th>
                                <th>Search Range</th>
                                <th>Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Learning Rate (lr)</td>
                                <td>[1e-4, 3e-4]</td>
                                <td>Policy and Q-function learning rate</td>
                            </tr>
                            <tr>
                                <td>Temperature (τ)</td>
                                <td>[0.005, 0.01]</td>
                                <td>Soft update coefficient for target networks</td>
                            </tr>
                            <tr>
                                <td>Hidden Dimension</td>
                                <td>[128, 256]</td>
                                <td>Network hidden layer dimension</td>
                            </tr>
                            <tr>
                                <td>Alpha (α)</td>
                                <td>[0.1, 0.2]</td>
                                <td>Initial entropy regularization coefficient</td>
                            </tr>
                            <tr>
                                <td>Batch Size</td>
                                <td>256 (fixed)</td>
                                <td>Training batch size</td>
                            </tr>
                            <tr>
                                <td>Update Frequency</td>
                                <td>1 (fixed)</td>
                                <td>Policy/Q-function update frequency</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <p>
                    We evaluated 16 different hyperparameter configurations (2 × 2 × 2 × 2 grid search). The best configuration found was:
                </p>
                <ul class="feature-list">
                    <li><strong>Learning Rate:</strong> 3e-4</li>
                    <li><strong>Hidden Dimension:</strong> 256</li>
                    <li><strong>Temperature (τ):</strong> 0.01</li>
                    <li><strong>Alpha (α):</strong> 0.2</li>
                </ul>
                <p>
                    This configuration achieved a final 10% mean reward of 36.26 ± 6.59 over 400 training episodes, with an overall max reward of 40.53. All delay experiments were conducted using this optimal configuration to ensure fair comparisons.
                </p>
            </section>

            <!-- Experiments -->
            <section id="experiments" class="content-section">
                <h2>Experimental Setup</h2>
                <p>
                    We conduct three main experiments to systematically evaluate the impact of computation delays and the effectiveness 
                    of Real-Time RL approaches:
                </p>

                <h3>Experiment 1: Classical RL Baseline (No Action Conditioning)</h3>
                <p>
                    In our first experiment, we test classical RL methods without action conditioning in a real-world-like setting 
                    with simulated delays. This serves as our baseline, demonstrating how traditional RL algorithms (which assume 
                    instantaneous action computation) perform when deployed in scenarios with non-negligible computation delays.
                </p>
                <p>
                    We use standard Soft Actor-Critic (SAC) with the policy conditioned only on the current state: 
                    <em>π(a<sub>t</sub> | s<sub>t</sub>)</em>. The environment is run with fixed delays to simulate the time required 
                    for neural network forward passes.
                </p>

                <div class="table-container">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Delay (s)</th>
                                <th>Training Episodes</th>
                                <th>Final Avg Reward</th>
                                <th>Max Reward</th>
                                <th>Eval Avg Reward</th>
                                <th>Eval Reward Std</th>
                                <th>Eval Avg Length</th>
                                <th>Training Time (h)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>0.010</td>
                                <td>200</td>
                                <td>11.65</td>
                                <td>12.35</td>
                                <td>13.98</td>
                                <td>0.137</td>
                                <td>2000.0</td>
                                <td>11.82</td>
                            </tr>
                            <tr>
                                <td>0.033</td>
                                <td>200</td>
                                <td>10.86</td>
                                <td>11.79</td>
                                <td>12.71</td>
                                <td>0.136</td>
                                <td>2000.0</td>
                                <td>9.48</td>
                            </tr>
                            <tr>
                                <td>0.050</td>
                                <td>200</td>
                                <td>10.86</td>
                                <td>12.04</td>
                                <td>13.38</td>
                                <td>0.103</td>
                                <td>2000.0</td>
                                <td>13.70</td>
                            </tr>
                            <tr>
                                <td>0.100</td>
                                <td>200</td>
                                <td>5.28</td>
                                <td>12.18</td>
                                <td>-2.03</td>
                                <td>1.230</td>
                                <td>2000.0</td>
                                <td>14.13</td>
                            </tr>
                            <tr>
                                <td>0.500</td>
                                <td>200</td>
                                <td>12.75</td>
                                <td>23.15</td>
                                <td>19.11</td>
                                <td>11.097</td>
                                <td>2000.0</td>
                                <td>12.92</td>
                            </tr>
                            <tr>
                                <td>1.000</td>
                                <td>200</td>
                                <td>-1.03</td>
                                <td>-0.94</td>
                                <td>-0.98</td>
                                <td>0.009</td>
                                <td>10.8</td>
                                <td>0.31</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <div class="plot-container" style="display: flex; flex-direction: column; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Figure E1.1: Reward vs. Delay - Classical RL (Baseline)</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/reward_vs_delay.png" alt="Reward vs Delay for Classical RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Evaluation and training rewards as a function of delay for classical RL without action conditioning. Note the failure at 0.1s delay (negative reward). The 1.0s delay shows no learning within 200 episodes (see Limitations section).</p>
                    </div>
                    
                    <div>
                        <h4>Figure E1.2: Learning Curves - Classical RL (Baseline)</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/learning_curves.png" alt="Learning Curves for Classical RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Training progress over 200 episodes for different delay values. Shows the degradation in classical RL performance as delays increase.</p>
                    </div>
                    
                    <div>
                        <h4>Figure E1.3: Episode Length vs. Delay - Classical RL</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/episode_length_vs_delay.png" alt="Episode Length vs Delay for Classical RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Average episode length (in steps) as a function of delay. Classical RL maintains full episode lengths (2000 steps) for smaller delays but fails at 0.1s and 1.0s delays.</p>
                    </div>
                    
                    <div>
                        <h4>Figure E1.4: Performance Distributions - Classical RL</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/performance_distributions.png" alt="Performance Distributions for Classical RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Distribution of episode rewards and lengths across different delay values. Shows the variance and consistency (or lack thereof) in classical RL performance.</p>
                    </div>
                    
                    <div>
                        <h4>Figure E1.5: Loss Convergence - Classical RL</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/loss_convergence.png" alt="Loss Convergence for Classical RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Convergence patterns for Q1, Q2, Policy, and Alpha losses during training. Shows how loss evolution varies with different delay values.</p>
                    </div>
                </div>

                <h3>Experiment 2: Real-Time RL with Fixed Delay (Action Conditioning)</h3>
                <p>
                    Our second experiment implements Real-Time RL by conditioning the policy on both the previous state and previous 
                    action: <em>π(a<sub>t</sub> | s<sub>t-1</sub>, a<sub>t-1</sub>)</em>. This allows the policy to learn to predict 
                    state evolution and compensate for the delay.
                </p>
                <p>
                    We test this approach with fixed delays to directly compare against the classical RL baseline and evaluate whether 
                    action conditioning improves performance under delay constraints.
                </p>

                <div class="table-container">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Delay (s)</th>
                                <th>Training Episodes</th>
                                <th>Final Avg Reward</th>
                                <th>Max Reward</th>
                                <th>Eval Avg Reward</th>
                                <th>Eval Reward Std</th>
                                <th>Eval Avg Length</th>
                                <th>Training Time (h)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>0.010</td>
                                <td>200</td>
                                <td>11.85</td>
                                <td>14.78</td>
                                <td>14.81</td>
                                <td>0.092</td>
                                <td>2000.0</td>
                                <td>9.15</td>
                            </tr>
                            <tr>
                                <td>0.033</td>
                                <td>200</td>
                                <td>12.02</td>
                                <td>12.32</td>
                                <td>15.37</td>
                                <td>0.134</td>
                                <td>2000.0</td>
                                <td>11.97</td>
                            </tr>
                            <tr style="opacity: 0.8; font-style: italic;">
                                <td>0.050*</td>
                                <td>200**</td>
                                <td>N/A</td>
                                <td>N/A</td>
                                <td>15.45</td>
                                <td>0.187</td>
                                <td>2000.0</td>
                                <td>N/A</td>
                            </tr>
                            <tr>
                                <td>0.100</td>
                                <td>200</td>
                                <td>6.01</td>
                                <td>14.94</td>
                                <td>12.17</td>
                                <td>0.177</td>
                                <td>2000.0</td>
                                <td>8.80</td>
                            </tr>
                            <tr>
                                <td>0.500</td>
                                <td>200</td>
                                <td>10.08</td>
                                <td>24.40</td>
                                <td><strong>27.77</strong></td>
                                <td>0.039</td>
                                <td>2000.0</td>
                                <td>10.20</td>
                            </tr>
                            <tr>
                                <td>1.000</td>
                                <td>200</td>
                                <td>-1.03</td>
                                <td>-0.93</td>
                                <td>-0.99</td>
                                <td>0.005</td>
                                <td>6.0</td>
                                <td>0.32</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <p style="font-size: 0.9em; color: #666; margin-top: 0.5rem;">
                    <strong>Note:</strong> * 0.050s delay: Training metrics file was lost (see Limitations section). Evaluation metrics are available. ** Episode count inferred from checkpoints.
                </p>
                
                <p>
                    <strong>Key Observations:</strong> Real-Time RL with action conditioning shows improved performance compared to classical RL, particularly at moderate delays (0.033s, 0.1s). Most notably, at 0.5s delay, Real-Time RL achieves an evaluation reward of 27.77 ± 0.039, significantly outperforming classical RL's 19.11 ± 11.097. This demonstrates that action conditioning enables the agent to better anticipate state changes during computation delays. However, both approaches fail completely at 1.0s delays, indicating a critical threshold beyond which the task becomes intractable.
                </p>
                
                <div class="plot-container" style="display: flex; flex-direction: column; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Figure E2.1: Reward vs. Delay - Real-Time RL (Action Conditioning)</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/reward_vs_delay.png" alt="Reward vs Delay for Real-Time RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Evaluation and training rewards as a function of delay for Real-Time RL with action conditioning. Note the unexpectedly high performance at 0.5s delay (27.77 reward) compared to smaller delays—this anomaly requires further investigation (see Limitations section). The 1.0s delay shows no learning within 200 episodes. Note: Training metrics for 0.05s delay are missing (see Limitations section).</p>
                    </div>
                    
                    <div>
                        <h4>Figure E2.2: Learning Curves - Real-Time RL (Action Conditioning)</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/learning_curves.png" alt="Learning Curves for Real-Time RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Training progress over 200 episodes for different delay values. Shows how Real-Time RL adapts to various delay magnitudes during training. Note: Training curve for 0.05s delay is unavailable due to missing training metrics.</p>
                    </div>
                    
                    <div>
                        <h4>Figure E2.3: Episode Length vs. Delay - Real-Time RL</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/episode_length_vs_delay.png" alt="Episode Length vs Delay for Real-Time RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Average episode length (in steps) as a function of delay. Real-Time RL maintains full episode lengths (2000 steps) across all delays where learning occurs, demonstrating better robustness than classical RL.</p>
                    </div>
                    
                    <div>
                        <h4>Figure E2.4: Performance Distributions - Real-Time RL</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/performance_distributions.png" alt="Performance Distributions for Real-Time RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Distribution of episode rewards and lengths across different delay values, showing the stability and consistency of Real-Time RL performance. Notice the tighter distributions and higher median rewards compared to classical RL.</p>
                    </div>
                    
                    <div>
                        <h4>Figure E2.5: Loss Convergence - Real-Time RL</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/loss_convergence.png" alt="Loss Convergence for Real-Time RL" style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;">
                        <p class="plot-caption">Convergence patterns for Q1, Q2, Policy, and Alpha losses during training. Shows how loss evolution varies with different delay values. Note: Loss data for 0.05s delay is unavailable due to missing training metrics.</p>
                    </div>
                </div>

                <h3>Experiment 3: Real-Time RL with Variable Time Delay</h3>
                <p>
                    Fixed-delay experiments provide a clean way to measure how performance changes as the average computation delay increases, but real systems rarely exhibit a single constant delay. In practice, inference time fluctuates due to background load, scheduling jitter, and occasional stalls. This third experiment focuses on robustness under variable delay by sampling a new delay value at each environment step.
                </p>
                <p>
                    We evaluated three mean delays (0.05s, 0.1s, 0.2s). For each mean we trained an action-conditioned SAC agent under five delay configurations: fixed delay, uniform jitter, truncated normal jitter, a heavy-tailed lognormal distribution, and a bursty mixture with rare spikes. The goal was to cover a diverse range of delay distributions from jitter to tail-heavy scenarios under limited compute.
                </p>
                <p>
                    Even though the policy is not explicitly told the delay value, it experiences the consequences of delay variability through state transitions. Over training it can learn a strategy that is robust in expectation over that distribution: smoother action changes, more conservative steering and throttle, and behaviors that keep the vehicle close to the lane center so occasional long delays are less likely to cause irrecoverable drift. With action conditioning, the previous action also provides context that can help the model learn under this new dynamic.
                </p>
            </section>

            <!-- Results -->
            <section id="results" class="content-section">
                <h2>Results and Analysis</h2>
                <p>
                    The results from our experiments will be presented here, including quantitative comparisons and qualitative 
                    analysis of agent behavior. Key metrics include:
                </p>
                <ul class="feature-list">
                    <li>Cumulative reward and task completion rates</li>
                    <li>Learning efficiency and sample complexity</li>
                    <li>Robustness to delay variations</li>
                    <li>Comparison between classical and Real-Time RL approaches</li>
                </ul>

                <h3>Performance Comparison Across Delays</h3>
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Delay (s)</th>
                                <th>Method</th>
                                <th>Eval Avg Reward</th>
                                <th>Eval Reward Std</th>
                                <th>Eval Avg Length</th>
                                <th>Performance Gap</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>0.010</td>
                                <td>Classical RL</td>
                                <td>13.98</td>
                                <td>0.137</td>
                                <td>2000.0</td>
                                <td>Baseline</td>
                            </tr>
                            <tr>
                                <td>0.010</td>
                                <td>Real-Time RL</td>
                                <td>14.81</td>
                                <td>0.092</td>
                                <td>2000.0</td>
                                <td>+5.9%</td>
                            </tr>
                            <tr>
                                <td>0.033</td>
                                <td>Classical RL</td>
                                <td>12.71</td>
                                <td>0.136</td>
                                <td>2000.0</td>
                                <td>Baseline</td>
                            </tr>
                            <tr>
                                <td>0.033</td>
                                <td>Real-Time RL</td>
                                <td>15.37</td>
                                <td>0.134</td>
                                <td>2000.0</td>
                                <td>+20.9%</td>
                            </tr>
                            <tr>
                                <td>0.100</td>
                                <td>Classical RL</td>
                                <td>-2.03</td>
                                <td>1.230</td>
                                <td>2000.0</td>
                                <td>Baseline (failed)</td>
                            </tr>
                            <tr>
                                <td>0.100</td>
                                <td>Real-Time RL</td>
                                <td>12.17</td>
                                <td>0.177</td>
                                <td>2000.0</td>
                                <td>+700% (recovery)</td>
                            </tr>
                            <tr>
                                <td>0.500</td>
                                <td>Classical RL</td>
                                <td>19.11</td>
                                <td>11.097</td>
                                <td>2000.0</td>
                                <td>Baseline</td>
                            </tr>
                            <tr>
                                <td>0.500</td>
                                <td>Real-Time RL</td>
                                <td><strong>27.77</strong></td>
                                <td>0.039</td>
                                <td>2000.0</td>
                                <td><strong>+45.3%</strong></td>
                            </tr>
                            <tr style="opacity: 0.6; font-style: italic;">
                                <td>1.000*</td>
                                <td>Classical RL</td>
                                <td>-0.98</td>
                                <td>0.009</td>
                                <td>10.8</td>
                                <td>Insufficient training</td>
                            </tr>
                            <tr style="opacity: 0.6; font-style: italic;">
                                <td>1.000*</td>
                                <td>Real-Time RL</td>
                                <td>-0.99</td>
                                <td>0.005</td>
                                <td>6.0</td>
                                <td>Insufficient training</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <p style="font-size: 0.9em; color: #666; margin-top: 0.5rem;">
                    <strong>Note:</strong> * 1.000s delay: Both methods failed to learn within 200 training episodes (see Limitations section).
                </p>
                
                <h3>Visual Comparison: Classical RL vs. Real-Time RL</h3>
                
                <div class="plot-container" style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Classical RL - Reward vs. Delay</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/reward_vs_delay.png" alt="Classical RL Reward vs Delay" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                    <div>
                        <h4>Real-Time RL - Reward vs. Delay</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/reward_vs_delay.png" alt="Real-Time RL Reward vs Delay" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                </div>
                <p class="plot-caption" style="text-align: center; margin-top: 1rem;">
                    <strong>Figure 6:</strong> Side-by-side comparison of reward vs. delay for both approaches. Real-Time RL maintains higher rewards and succeeds at 0.1s delay where classical RL fails, and achieves superior performance (27.77 vs. 19.11) at 0.5s delay.
                </p>
                
                <div class="plot-container" style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Classical RL - Episode Length vs. Delay</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/episode_length_vs_delay.png" alt="Classical RL Episode Length vs Delay" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                    <div>
                        <h4>Real-Time RL - Episode Length vs. Delay</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/episode_length_vs_delay.png" alt="Real-Time RL Episode Length vs Delay" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                </div>
                <p class="plot-caption" style="text-align: center; margin-top: 1rem;">
                    <strong>Figure 7:</strong> Episode length comparison. Both approaches maintain full episode lengths (2000 steps) for delays up to 0.5s. The 1.0s delay results show very short episodes (6-10 steps), but this likely reflects insufficient training time rather than a fundamental failure (see Limitations section).
                </p>
                
                <div class="plot-container" style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Classical RL - Learning Curves</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/learning_curves.png" alt="Classical RL Learning Curves" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                    <div>
                        <h4>Real-Time RL - Learning Curves</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/learning_curves.png" alt="Real-Time RL Learning Curves" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                </div>
                <p class="plot-caption" style="text-align: center; margin-top: 1rem;">
                    <strong>Figure 8:</strong> Training progress comparison. Real-Time RL shows more stable learning and higher final rewards across all delay values, particularly evident at 0.1s and 0.5s delays. Note: The 0.05s delay condition in Real-Time RL is missing training metrics data (see Limitations section), so only evaluation metrics are available for this condition.
                </p>
                
                <div class="plot-container" style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Classical RL - Performance Distributions</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/performance_distributions.png" alt="Classical RL Performance Distributions" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                    <div>
                        <h4>Real-Time RL - Performance Distributions</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/performance_distributions.png" alt="Real-Time RL Performance Distributions" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                </div>
                <p class="plot-caption" style="text-align: center; margin-top: 1rem;">
                    <strong>Figure 9:</strong> Performance distribution comparison. Real-Time RL shows tighter distributions and higher median rewards, indicating more consistent and reliable performance, especially at higher delay values.
                </p>
                
                <div class="plot-container" style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Classical RL - Loss Convergence</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_no_action/analysis/loss_convergence.png" alt="Classical RL Loss Convergence" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                    <div>
                        <h4>Real-Time RL - Loss Convergence</h4>
                        <img src="gym-duckiematrix/delay_experiments/experiments_20251218_130600/analysis/loss_convergence.png" alt="Real-Time RL Loss Convergence" style="width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                    </div>
                </div>
                <p class="plot-caption" style="text-align: center; margin-top: 1rem;">
                    <strong>Figure 10:</strong> Loss convergence patterns. Both approaches show similar loss evolution, but Real-Time RL achieves lower final losses, particularly for policy and Q-function losses, indicating better optimization. Note: Loss convergence data for the 0.05s delay in Real-Time RL is unavailable due to missing training metrics (see Limitations section).
                </p>

                <h3>Experiment 3: Variable Delay Robustness</h3>
                <p>
                    Experiment 3 isolates two effects that are often conflated: increasing the mean delay, and the additional penalty introduced by variability and tail events. Across means, performance is expected to degrade as delays become larger. For the same mean, small bounded jitter should be less harmful than heavy tails, since rare long delays can produce very stale actions and abrupt lane departure.
                </p>

                <div class="plot-container" style="display: flex; flex-direction: column; gap: 2rem; margin: 2rem 0;">
                    <div>
                        <h4>Figure E3.1: Performance vs Mean Delay (lines = delay regime)</h4>
                        <img
                            src="gym-duckiematrix/delay_experiments/experiment_3_variable_delay/analysis/exp3_reward_vs_mean_by_regime.png"
                            alt="Experiment 3: performance vs mean delay"
                            style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;"
                            loading="lazy"
                        >
                        <p class="plot-caption">Evaluation reward across mean delays, with one curve per delay regime. Each point corresponds to a trained policy evaluated under the same delay regime it was trained on. Shows how different delay distributions (fixed, uniform jitter, truncated normal, lognormal, bursty) affect performance at various mean delays.</p>
                    </div>

                    <div>
                        <h4>Figure E3.2: Absolute Difference vs Fixed Baseline (lines = delay regime)</h4>
                        <img
                            src="gym-duckiematrix/delay_experiments/experiment_3_variable_delay/analysis/exp3_abs_delta_vs_fixed_across_means.png"
                            alt="Experiment 3: absolute delta vs fixed baseline"
                            style="width: 100%; max-width: 800px; border: 1px solid #ddd; border-radius: 8px;"
                            loading="lazy"
                        >
                        <p class="plot-caption">Absolute change in evaluation reward relative to the fixed-delay baseline at the same mean. Values below zero indicate that variability hurts performance; values above zero indicate that the variable-delay regime performed better than fixed at that mean. This helps isolate the impact of delay variability from the impact of increasing mean delay.</p>
                    </div>
                </div>

                <p>
                    The overall pattern matches expectations: increasing mean delay tends to reduce performance, and heavy-tailed regimes collapse as delay increases. For the same mean delay, bounded jitter is generally less harmful than heavy tails. We also observed that, at a 0.2s mean delay, some jitter regimes outperformed the fixed-delay run. Given that each configuration was trained once, this should be interpreted cautiously; it is also plausible that stochastic delay can regularize learning. A minimal follow-up would repeat the fixed and jitter conditions with multiple seeds to quantify how often this effect appears.
                </p>

                <h3>Key Findings</h3>
                <ul class="feature-list">
                    <li><strong>Action Conditioning Effectiveness:</strong> Real-Time RL consistently outperforms classical RL across all delay values where successful learning occurs. The performance gap widens as delays increase, reaching a 45.3% improvement at 0.5s delay.</li>
                    <li><strong>Recovery at Moderate Delays:</strong> At 0.1s delay, classical RL fails (negative reward), but Real-Time RL successfully learns with a reward of 12.17 ± 0.177, demonstrating that action conditioning can enable learning in scenarios where classical RL fails.</li>
                    <li><strong>Variance Reduction:</strong> Real-Time RL shows significantly lower variance in evaluation rewards (e.g., 0.039 at 0.5s vs. 11.097 for classical RL), indicating more stable and reliable performance.</li>
                    <li><strong>Training Efficiency:</strong> Both approaches require similar training times (8-14 hours for 200 episodes), suggesting that action conditioning does not significantly increase computational overhead.</li>
                </ul>
                
                <h3>Limitations and Open Questions</h3>
                
                <h4>1.0s Delay: Insufficient Training Time</h4>
                <p>
                    We note an important limitation in our experimental design: the 1.0s delay experiments show evaluation rewards near -1.0 and extremely short episode lengths (6-10 steps), suggesting that learning did not occur within the 200 training episodes allocated. This makes sense given that larger delays represent significantly more difficult learning scenarios—the agent must learn to predict state evolution over a much longer time horizon.
                </p>
                <p>
                    <strong>Implications:</strong> The 1.0s delay results should be interpreted with caution, as they likely represent incomplete learning rather than a true performance ceiling. With more training time (e.g., 500-1000 episodes), it is possible that meaningful learning could occur, though we expect performance would still be substantially degraded compared to smaller delays. Future work should extend training duration for larger delay values to properly characterize the learning dynamics and identify whether there exists a true hard threshold beyond which the task becomes intractable.
                </p>
                <p>
                    <strong>Current Status:</strong> Given computational constraints, we have not yet had the opportunity to extend training for the 1.0s delay condition. As such, these results are excluded from our performance comparisons and should be viewed as preliminary observations rather than conclusive findings.
                </p>
                
                <h4>The 0.5s Delay Anomaly: Unexpected Outperformance</h4>
                <p>
                    One of the most intriguing and unexpected observations from our experiments is that the 0.5s delay condition appears to outperform smaller delays, particularly in the Real-Time RL setting. At 0.5s delay, Real-Time RL achieves an evaluation reward of 27.77 ± 0.039, which is substantially higher than rewards at 0.01s (14.81), 0.033s (15.37), and 0.1s (12.17). This contradicts the intuitive expectation that performance should continuously degrade as delays increase.
                </p>
                <p>
                    <strong>Current Limitations:</strong> We have not yet had the opportunity to:
                </p>
                <ul class="feature-list">
                    <li>Rerun the 0.5s delay experiments multiple times to verify reproducibility</li>
                    <li>Extend training duration to see if smaller delays eventually match or exceed 0.5s performance</li>
                    <li>Investigate potential implementation bugs or evaluation artifacts that could explain this result</li>
                </ul>
                <p>
                    As such, we cannot yet determine whether this represents a genuine phenomenon, a statistical outlier, or a methodological artifact.
                </p>
                <p>
                    <strong>Possible Explanations (if the result is genuine):</strong>
                </p>
                <ul class="feature-list">
                    <li><strong>Optimal Exploration-Exploitation Balance:</strong> The 0.5s delay might create a "sweet spot" where the temporal delay encourages the agent to learn more robust, forward-looking policies. Smaller delays might allow the agent to rely on reactive policies that work well in the short term but are less optimal overall, while the 0.5s delay forces the agent to develop better predictive models.</li>
                    <li><strong>Action Smoothing Effect:</strong> Longer delays might act as an implicit regularization mechanism, smoothing out noisy or jerky actions that could occur with near-instantaneous control. This smoothing could lead to more stable and consistent behavior, particularly in a continuous control domain like autonomous driving.</li>
                    <li><strong>Hyperparameter Mismatch:</strong> Our hyperparameters were optimized through grid search, but this optimization may have been biased toward a particular delay range. The 0.5s delay might happen to align better with the chosen hyperparameter configuration, while smaller delays might benefit from different settings (e.g., different learning rates, network architectures, or exploration strategies).</li>
                    <li><strong>Curriculum Learning Effect:</strong> The 0.5s delay might provide an unintentional curriculum—the difficulty is high enough to force sophisticated learning, but not so high as to make learning impossible. Smaller delays might allow the agent to find suboptimal local minima too easily, while the 0.5s delay forces more thorough exploration of the policy space.</li>
                    <li><strong>Reward Function Interactions:</strong> The reward structure in Duckietown might interact with the delay in non-obvious ways. For example, longer delays might change the temporal structure of rewards in a way that makes certain behaviors more rewarding, or might affect how the agent balances immediate vs. long-term rewards.</li>
                </ul>
                <p>
                    <strong>Future Work:</strong> To resolve this anomaly, we plan to:
                </p>
                <ul class="feature-list">
                    <li>Run multiple independent training runs for the 0.5s delay condition to assess statistical significance</li>
                    <li>Perform hyperparameter optimization specifically for each delay value to eliminate potential bias</li>
                    <li>Conduct ablation studies to isolate which components of the algorithm or environment might contribute to this effect</li>
                    <li>Extend training duration for all delay values to see if the performance ordering changes with longer training</li>
                </ul>
                <p>
                    <strong>Current Recommendation:</strong> Until further verification, we recommend interpreting the 0.5s delay results with caution and focusing comparisons on the relative performance between Classical RL and Real-Time RL at each delay value, rather than comparing across delay values.
                </p>

                <h4>Missing Training Metrics for 0.05s Delay (Real-Time RL Experiment)</h4>
                <p>
                    During the Real-Time RL experiment with action conditioning, the training metrics file (<code>training_metrics.json</code>) for the 0.05s delay condition was lost or not properly saved. As a result, while evaluation metrics are available for this condition (showing an evaluation reward of 15.45 ± 0.187), we lack detailed training history including episode-by-episode rewards, losses, and learning curves for this specific delay value.
                </p>
                <p>
                    <strong>Impact:</strong> This missing data means that plots and analysis that depend on training metrics (such as detailed learning curves, loss convergence patterns, and training time analysis) show "N/A" or incomplete data for the 0.05s delay in the Real-Time RL experiment. The evaluation results remain valid and are included in our comparisons, but training progression data cannot be analyzed for this condition. The checkpoints for this experiment were successfully saved, confirming that training completed successfully—only the metrics logging file was affected.
                </p>
                
                <h4>Experiment 3: Limited Runs</h4>
                <p>
                    Variable-delay experiments expand the number of conditions quickly, since each combination of mean delay and delay regime requires a full training run. Due to limited computational time, we ran a single seed per configuration. This makes the results sensitive to training stochasticity and can plausibly explain some irregularities, such as a fixed-delay run underperforming a jittered counterpart at the same mean. Future work should repeat a subset of configurations across multiple seeds to provide confidence intervals on robustness under variable delay.
                </p>
            </section>

            <!-- Technical Details -->
            <section class="content-section">
                <h2>Technical Implementation</h2>
                <p>
                    The implementation leverages modern deep RL frameworks and optimization techniques. We utilized:
                </p>
                <div class="tech-stack">
                    <span class="tech-badge">PyTorch</span>
                    <span class="tech-badge">Gymnasium</span>
                    <span class="tech-badge">Duckietown</span>
                    <span class="tech-badge">Duckiematrix</span>
                    <span class="tech-badge">TensorRT</span>
                </div>
            </section>

            <!-- Conclusion -->
            <section id="conclusion" class="content-section">
                <h2>Conclusion</h2>
                <p>
                    Through our systematic investigation of Real-Time Reinforcement Learning in Duckietown, we aim to demonstrate 
                    the importance of accounting for computation delays in RL systems. Our experiments compare classical RL approaches 
                    (which assume instantaneous action computation) against Real-Time RL methods that condition on previous actions 
                    to learn state evolution predictions.
                </p>
                <p>
                    The results will provide insights into:
                </p>
                <ul class="feature-list">
                    <li><strong>Computation delays significantly degrade classical RL performance:</strong> At 0.1s delay, classical RL fails completely, and even at smaller delays, performance decreases by 9-21% compared to baseline.</li>
                    <li><strong>Action conditioning effectively compensates for delays:</strong> Real-Time RL achieves 20.9% better performance at 0.033s delay and 45.3% better at 0.5s delay compared to classical RL, with dramatically reduced variance.</li>
                    <li><strong>Limitations in current results:</strong> The 1.0s delay experiments did not show learning within 200 episodes and require further investigation with extended training durations (see Limitations section).</li>
                </ul>
                <p>
                    These findings have important implications for deploying RL systems in real-world, safety-critical applications 
                    where computation time cannot be ignored. Future work will explore more sophisticated delay compensation techniques 
                    and extend the approach to multi-agent scenarios in the Duckietown environment.
                </p>
            </section>

            <!-- References -->
            <section class="content-section">
                <h2>References</h2>
                <ul class="resource-list">
                    <li>[1] Ramstedt, S., & Pal, C. (2019). Real-Time Reinforcement Learning. <em>Advances in Neural Information Processing Systems</em>. <a href="https://arxiv.org/abs/1911.04448" target="_blank">arXiv:1911.04448</a></li>
                    <li>Haarnoja, T., et al. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.</li>
                    <li><a href="https://www.duckietown.org/" target="_blank">Duckietown Project</a></li>
                    <li><a href="https://github.com/duckietown" target="_blank">Duckietown GitHub</a></li>
                </ul>
            </section>
        </article>
    </main>

    <footer class="footer">
        <div class="footer-content">
            <p>&copy; 2024 Duckietown Research Project. All rights reserved.</p>
            <p class="footer-note">Built with ❤️ for autonomous driving research</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
